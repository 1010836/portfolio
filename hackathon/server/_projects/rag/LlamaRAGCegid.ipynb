{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1010836/portfolio/blob/main/hackathon/server/_projects/rag/LlamaRAGCegid.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zg8K_kYzLm2N"
      },
      "source": [
        "# setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RpW0myfJL95"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnssDktl9oTk"
      },
      "outputs": [],
      "source": [
        "!pip install torch --index-url https://download.pytorch.org/whl/cu117\n",
        "!pip install langchain einops accelerate transformers bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "An1bx5N1B6Ok"
      },
      "source": [
        "# definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rYUbji0B99S"
      },
      "outputs": [],
      "source": [
        "# Define variable to hold llama2 weights naming\n",
        "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "# Set auth token variable from hugging face\n",
        "auth_token = \"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\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPboLoMdBgrK"
      },
      "source": [
        "# model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTyNYnQSBnmD"
      },
      "outputs": [],
      "source": [
        "# 1. Import transformer Auto to load\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "# 2. Import torch for datatype attributes\n",
        "import torch\n",
        "\n",
        "# 3. Load model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    cache_dir='./model/',\n",
        "    token=auth_token,\n",
        "    torch_dtype=torch.float16,\n",
        "    rope_scaling={\"type\": \"dynamic\", \"factor\": 2},\n",
        "    load_in_8bit=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_Uz1asgL_qe"
      },
      "source": [
        "# tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMhPHPwlGktc"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, TextStreamer\n",
        "\n",
        "# 1. tokenizing (spliting strings in sub-word token strings), converting tokens strings to ids and back, and encoding/decoding (i.e. tokenizing + convert to integers),\n",
        "# 2. adding new tokens to the vocabulary in a way that is independant of the underlying structure (BPE, SentencePieceâ€¦),\n",
        "# 3. managing special tokens like mask, beginning-of-sentence, etc tokens (adding them, assigning them to attributes in the tokenizer for easy access and making sure they are not split during tokenization)\n",
        "# 4. https://huggingface.co/transformers/v3.0.2/main_classes/tokenizer.html\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    cache_dir='./model/',\n",
        "    token=auth_token\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vqzzRenk52X"
      },
      "source": [
        "# prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AlHaMeOqnKo"
      },
      "outputs": [],
      "source": [
        "from llama_index.prompts.prompts import SimpleInputPrompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAsrn-19lCvw"
      },
      "outputs": [],
      "source": [
        "# Import the prompt wrapper...but for llama index\n",
        "# from llama_index.prompts.prompts import SimpleInputPrompt\n",
        "# Create a system prompt\n",
        "system_prompt = \"\"\"[INST] <>\n",
        "You are a helpful, respectful and honest assistant. Always answer as\n",
        "helpfully as possible, while being safe. Your answers should not include\n",
        "any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\n",
        "Please ensure that your responses are socially unbiased and positive in nature.\n",
        "\n",
        "If a question does not make any sense, or is not factually coherent, explain\n",
        "why instead of answering something not correct. If you don't know the answer\n",
        "to a question, please don't share false information.\n",
        "\n",
        "Your goal is to provide answers relating to the software CEGID Talentsof.<>\n",
        "\"\"\"\n",
        "# Throw together the query wrapper\n",
        "query_wrapper_prompt = SimpleInputPrompt(\"{query_str} [/INST]\")\n",
        "\n",
        "# Complete the query prompt\n",
        "query_wrapper_prompt.format(query_str='hello')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGvr8ieymkMv"
      },
      "source": [
        "# LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "briorjyWmbo3"
      },
      "outputs": [],
      "source": [
        "# Import the llama index HF Wrapper\n",
        "from llama_index.llms import HuggingFaceLLM\n",
        "# Create a HF LLM using the llama index wrapper\n",
        "llm = HuggingFaceLLM(context_window=4096,\n",
        "                    max_new_tokens=256,\n",
        "                    system_prompt=system_prompt,\n",
        "                    query_wrapper_prompt=query_wrapper_prompt,\n",
        "                    model=model,\n",
        "                    tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KavOdRV_m0zU"
      },
      "source": [
        "# embendings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i566U0EgtVbk"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixaGxGUEm3G2"
      },
      "outputs": [],
      "source": [
        "# Bring in embeddings wrapper\n",
        "from llama_index.embeddings import LangchainEmbedding\n",
        "# Bring in HF embeddings - need these to represent document chunks\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "# Create and dl embeddings instance\n",
        "embeddings=LangchainEmbedding(\n",
        "    HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brjx4tXanV0f"
      },
      "source": [
        "# context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ef0_Uqa_nYLG"
      },
      "outputs": [],
      "source": [
        "# Bring in stuff to change service context\n",
        "from llama_index import set_global_service_context\n",
        "from llama_index import ServiceContext\n",
        "\n",
        "# Create new service context instance\n",
        "service_context = ServiceContext.from_defaults(\n",
        "    chunk_size=1024,\n",
        "    llm=llm,\n",
        "    embed_model=embeddings\n",
        ")\n",
        "# And set the service context\n",
        "set_global_service_context(service_context)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCjS8uKSnocH"
      },
      "source": [
        "# documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKwWdTz3nqf6"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "link = \"https://huggingface.co/datasets/E1010836/rag/raw/main/cegid_talentsof_QeA.txt\"\n",
        "f = requests.get(link)\n",
        "f.encoding = 'UTF-16'\n",
        "documents = f.text\n",
        "with open('q&a.txt', 'w') as f:\n",
        "    f.write(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ok2KGPI0dI-"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from llama_index import download_loader\n",
        "\n",
        "UnstructuredReader = download_loader(\"UnstructuredReader\", refresh_cache=True)\n",
        "loader = UnstructuredReader()\n",
        "documents = loader.load_data(file=Path('./q&a.txt'))\n",
        "\n",
        "print(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2m9xwmMoY4o"
      },
      "source": [
        "# index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RWYs-OL01vN"
      },
      "outputs": [],
      "source": [
        "from llama_index import VectorStoreIndex\n",
        "\n",
        "# Create an index - we'll be able to query this in a sec\n",
        "index = VectorStoreIndex.from_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pkR60QxckOq"
      },
      "source": [
        "# API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UBamvVqyury"
      },
      "outputs": [],
      "source": [
        "!pip install pyngrok\n",
        "!pip install flask\n",
        "!pip install -U flask-cors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqxYe1r5yY5p"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "import os\n",
        "import threading\n",
        "from pyngrok import ngrok\n",
        "from flask import Flask\n",
        "from flask import jsonify\n",
        "from flask import request\n",
        "from flask_cors import CORS\n",
        "import json\n",
        "\n",
        "!ngrok config add-authtoken 2V1aDZKTFokAKx9aIuIp6l6GbYz_7mVxXhFHMcqs1RZ9B4RVE\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "port = 5000\n",
        "\n",
        "# Open a ngrok tunnel to the HTTP server\n",
        "public_url = ngrok.connect(port).public_url\n",
        "print(\" * ngrok tunnel \\\"{}\\\" -> \\\"http://127.0.0.1:{}\\\"\".format(public_url, port))\n",
        "\n",
        "# Your API endpoint\n",
        "url = 'http://51.20.133.14/store'\n",
        "\n",
        "# Your data to send, this is an example, replace with your actual data\n",
        "data = {\n",
        "    \"key\": \"address\",\n",
        "    \"value\": public_url\n",
        "}\n",
        "\n",
        "# Convert the data to JSON format\n",
        "data_json = json.dumps(data)\n",
        "\n",
        "# Your headers\n",
        "headers = {\n",
        "    'Content-Type': 'application/json',\n",
        "}\n",
        "\n",
        "# Send the POST request\n",
        "response = requests.post(url, data=data_json, headers=headers)\n",
        "\n",
        "# Print the response\n",
        "print(response.json())\n",
        "\n",
        "# Update any base URLs to use the public ngrok URL\n",
        "app.config[\"BASE_URL\"] = public_url\n",
        "\n",
        "# Define Flask routes\n",
        "@app.route(\"/\")\n",
        "def text():\n",
        "    question = request.args.get('question')\n",
        "\n",
        "    # global index\n",
        "    query_engine = index.as_query_engine()\n",
        "    response = query_engine.query(question)\n",
        "\n",
        "    response = str(response)\n",
        "    cleanResponse = response.replace('\"', '')\n",
        "    cleanResponse = cleanResponse.replace('\\n\\n', '')\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"-------------------------------------------------------\")\n",
        "    print(cleanResponse)\n",
        "    print(\"-------------------------------------------------------\")\n",
        "\n",
        "    result = jsonify({\"answer\": cleanResponse})\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"-------------------------------------------------------\")\n",
        "    print(result)\n",
        "    print(\"-------------------------------------------------------\")\n",
        "\n",
        "    return result\n",
        "\n",
        "# Start the Flask server in a new thread\n",
        "# threading.Thread(target=app.run, kwargs={\"use_reloader\": False}).start()\n",
        "app.run()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMclCHq0ph0HTzGXTcITgB9",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}